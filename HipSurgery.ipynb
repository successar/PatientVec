{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Run Diagnosis experiments')\n",
    "parser.add_argument(\"--data_dir\", type=str, required=True)\n",
    "parser.add_argument('--display', dest='display', action='store_true')\n",
    "parser.add_argument(\"--output_dir\", type=str)\n",
    "parser.add_argument(\"--mock\", dest='mock', action='store_true')\n",
    "\n",
    "args = parser.parse_args(['--data_dir=.', '--output_dir=outputs/', '--display'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.models.Vanilla import ClassificationTrainer as BasicCT\n",
    "from PatientVec.models.Hierarchical import ClassificationTrainer as HierCT\n",
    "from PatientVec.trainer import Trainer, Evaluator\n",
    "from PatientVec.Experiments.modifiable_config_exp import vanilla_configs, attention_configs, hierarchical_configs, structured_configs\n",
    "\n",
    "from PatientVec.Experiments.hyperparam_exps import get_basic_data\n",
    "from PatientVec.models.baselines.LR import LR, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import hip_dataset, knee_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in [1, 2, 3] :\n",
    "    data = hip_dataset(args, yr=yr)\n",
    "#     train_data, dev_data = get_basic_data(data, structured=False, truncate=100)\n",
    "\n",
    "#     lr = LR({'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name, 'type' : 'classifier', 'norm' : 'l2'})\n",
    "#     lr.train(train_data)\n",
    "#     lr.evaluate(dev_data, save_results=True)\n",
    "#     try :\n",
    "#         lr.print_all_features(n=30)\n",
    "#     except :\n",
    "#         pass\n",
    "        \n",
    "#     print('=' * 200)\n",
    "\n",
    "    train_data, dev_data = get_basic_data(data, structured=False, truncate=98)\n",
    "\n",
    "    for e in attention_configs :\n",
    "        config = e(data, structured=False, args=args)\n",
    "        if args.output_dir is not None :\n",
    "            config['exp_config']['basepath'] = args.output_dir\n",
    "        config['training_config']['common']['bsize'] = 8\n",
    "        config['training_config']['common']['class_weight'] = False\n",
    "        config['training_config']['common']['balanced'] = True\n",
    "        config['exp_config']['exp_name'] += '+Balanced'\n",
    "        config['training_config']['type'] = 'RMSprop'\n",
    "        print(config)\n",
    "\n",
    "        trainer = Trainer(BasicCT, config, _type=data.metrics_type, display_metrics=True)\n",
    "        trainer.train(train_data, dev_data, n_iters=15, save_on_metric=data.save_on_metric)\n",
    "\n",
    "        evaluator = Evaluator(BasicCT, trainer.model.dirname, _type=data.metrics_type, display_metrics=True)\n",
    "        _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "        print('-'*300)\n",
    "\n",
    "    for e in vanilla_configs :\n",
    "        config = e(data, structured=False, args=args)\n",
    "        if args.output_dir is not None :\n",
    "            config['exp_config']['basepath'] = args.output_dir\n",
    "        config['training_config']['common']['bsize'] = 8\n",
    "        config['training_config']['common']['class_weight'] = False\n",
    "        config['training_config']['common']['balanced'] = True\n",
    "        config['exp_config']['exp_name'] += '+Balanced'\n",
    "        print(config)\n",
    "\n",
    "        trainer = Trainer(BasicCT, config, _type=data.metrics_type, display_metrics=True)\n",
    "        trainer.train(train_data, dev_data, n_iters=15, save_on_metric=data.save_on_metric)\n",
    "\n",
    "        evaluator = Evaluator(BasicCT, trainer.model.dirname, _type=data.metrics_type, display_metrics=True)\n",
    "        _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "        print('-'*300)\n",
    "        \n",
    "    print('='*500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in [1, 2, 3] :\n",
    "    data = hip_dataset(args, yr=yr)\n",
    "#     train_data, dev_data = get_basic_data(data, structured=False, truncate=100)\n",
    "\n",
    "#     lr = LR({'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name, 'type' : 'classifier', 'norm' : 'l2'})\n",
    "#     lr.train(train_data)\n",
    "#     lr.evaluate(dev_data, save_results=True)\n",
    "#     try :\n",
    "#         lr.print_all_features(n=30)\n",
    "#     except :\n",
    "#         pass\n",
    "        \n",
    "#     print('=' * 200)\n",
    "\n",
    "    train_data, dev_data = get_basic_data(data, structured=False, truncate=98)\n",
    "\n",
    "    for e in vanilla_configs :\n",
    "        config = e(data, structured=False, args=args)\n",
    "        if args.output_dir is not None :\n",
    "            config['exp_config']['basepath'] = args.output_dir\n",
    "        config['training_config']['common']['bsize'] = 8\n",
    "        config['model']['predictor']['replicate'] = True\n",
    "        config['model']['predictor']['alpha'] = 0.3\n",
    "        config['exp_config']['exp_name'] += '+Replicate'\n",
    "        config['training_config']['type'] = 'RMSprop'\n",
    "        print(config)\n",
    "\n",
    "        trainer = Trainer(BasicCT, config, _type=data.metrics_type, display_metrics=True)\n",
    "        trainer.train(train_data, dev_data, n_iters=15, save_on_metric=data.save_on_metric)\n",
    "\n",
    "        evaluator = Evaluator(BasicCT, trainer.model.dirname, _type=data.metrics_type, display_metrics=True)\n",
    "        _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "        print('-'*300)\n",
    "        \n",
    "    print('='*500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in [1, 2, 3] :\n",
    "    data = hip_dataset(args, yr=yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import generate_latex_tables\n",
    "keys_to_use = ['accuracy', 'roc_auc', 'pr_auc']\n",
    "for yr in [1, 2, 3] :\n",
    "    data = knee_dataset(args, yr=yr)\n",
    "    generate_latex_tables(data, keys_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['Attention', 'baselines', 'Basic'] :\n",
    "    yr_df = {}\n",
    "    for yr in [1, 2, 3] :\n",
    "        df = pd.read_csv('Text-encoding-EHR/results/HipSurgery_' + str(yr) + '/' + model + '.csv')\n",
    "        df.index = df['Method']\n",
    "        df = df.drop(columns=['Method'])\n",
    "        yr_df[yr] = df\n",
    "    yr_df = pd.concat(yr_df.values(), axis=1, keys=yr_df.keys())\n",
    "    yr_df.columns = yr_df.columns.swaplevel(0, 1)\n",
    "    yr_df.sort_index(axis=1, level=0, inplace=True)\n",
    "    display(HTML(yr_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['Attention', 'baselines', 'Basic'] :\n",
    "    yr_df = {}\n",
    "    for yr in [1, 2, 3] :\n",
    "        df = pd.read_csv('Text-encoding-EHR/results/KneeSurgery_' + str(yr) + '/' + model + '.csv')\n",
    "        df.index = df['Method']\n",
    "        df = df.drop(columns=['Method'])\n",
    "        yr_df[yr] = df\n",
    "    yr_df = pd.concat(yr_df.values(), axis=1, keys=yr_df.keys())\n",
    "    yr_df.columns = yr_df.columns.swaplevel(0, 1)\n",
    "    yr_df.sort_index(axis=1, level=0, inplace=True)\n",
    "    display(HTML(yr_df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
