{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common import *\n",
    "from PatientVec.Experiments.hyperparam_exps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Run Diagnosis experiments')\n",
    "parser.add_argument(\"--data_dir\", type=str, required=True)\n",
    "parser.add_argument('--display', dest='display', action='store_true')\n",
    "parser.add_argument(\"--output_dir\", type=str)\n",
    "parser.add_argument(\"--mock\", dest='mock', action='store_true')\n",
    "\n",
    "args = parser.parse_args(['--data_dir=.', '--output_dir=outputs/', '--display'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import readmission_dataset, mortality_dataset\n",
    "data = readmission_dataset(args)\n",
    "# data = mortality_dataset(args, _type='30day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [500, 1000, 1500, 2000, 3000] :\n",
    "    args.n = n\n",
    "    experiment_types['ts_experiments'](data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured = True\n",
    "train_data, dev_data = get_basic_data(data, structured=structured, truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(set(range(len(train_data.X))) - set([1249]))\n",
    "train_data = train_data.filter(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = sru_configs[0](data, structured=structured, args=args)\n",
    "if args.output_dir is not None :\n",
    "    config['exp_config']['basepath'] = args.output_dir\n",
    "print(config)\n",
    "\n",
    "trainer = Trainer(BasicCT, config, _type=data.metrics_type, display_metrics=args.display)\n",
    "trainer.train(train_data, dev_data, n_iters=10, save_on_metric=data.save_on_metric)\n",
    "\n",
    "evaluator = Evaluator(BasicCT, trainer.model.dirname, _type=data.metrics_type, display_metrics=args.display)\n",
    "_ = evaluator.evaluate(dev_data, save_results=True)\n",
    "print('='*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data = get_basic_data(data, structured=True, truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auc = {}\n",
    "results_pr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auc_l1 = {}\n",
    "results_pr_l1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [1, 2, 3] :\n",
    "    config = {'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name, \n",
    "              'type' : data.metrics_type, 'norm' : 'l'+str(l), 'constant_mul' : 1.0}\n",
    "    lr = LR(config)\n",
    "    lr.train(train_data)\n",
    "    metrics = lr.evaluate(dev_data, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_models = {}\n",
    "for c in [1.0, 2.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0, 10000, 100000] :\n",
    "    config = {'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name, \n",
    "              'type' : data.metrics_type, 'norm' : 'l1', 'constant_mul' : c}\n",
    "    lr = LR(config)\n",
    "    lr.train(train_data)\n",
    "    metrics = lr.evaluate(dev_data, save_results=True)\n",
    "    results_auc_l1[c] = metrics['roc_auc']\n",
    "    results_pr_l1[c] = metrics['pr_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logx = list(results_auc.keys())\n",
    "plt.plot(logx, list(results_auc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pr_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_feat = lr_models[1].bow_classifier.estimators_[0].coef_[0]\n",
    "l2_feat = lr_models[2].bow_classifier.estimators_[0].coef_[0]\n",
    "l3_feat = lr_models[3].bow_classifier.estimators_[0].coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(sorted(l1_feat))\n",
    "sns.kdeplot(sorted(l2_feat))\n",
    "sns.kdeplot(sorted(l3_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.filter_data_length(data.get_data('train', structured=True), 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_dir = '../../../SurgeryData/PatientVec/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import hip_dataset\n",
    "data = hip_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.Experiments.evaluate import get_evaluator\n",
    "train_data, dev_data = get_basic_data(data, structured=False, truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = get_evaluator(data, 'Basic/LSTM(hs=128)/')\n",
    "output = evaluator.evaluate(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name, \n",
    "          'type' : data.metrics_type, 'norm' : 'l2', 'constant_mul' : 1.0}\n",
    "lr = LR(config)\n",
    "lr.train(train_data)\n",
    "metrics = lr.evaluate(dev_data, save_results=True)\n",
    "pred = lr.predict(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_y = output['predictions'][:, 0]\n",
    "lr_y = pred[:, 1]\n",
    "true_y = np.array(dev_data.y)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lstm_y, lr_y, s=1, c=true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0.0, 0.25, 0.5, 0.75]\n",
    "results = {}\n",
    "rocs = {}\n",
    "for b in bins :\n",
    "    idx = np.where(np.logical_and(lr_y >= b, lr_y < b+0.25))[0]\n",
    "    results[b] = {'lr' : lr_y[idx], 'lstm' : lstm_y[idx], 'true' : true_y[idx]}\n",
    "    rocs[b] = {'lr' : roc_auc_score(true_y[idx], lr_y[idx]), 'lstm' : roc_auc_score(true_y[idx], lstm_y[idx])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_end = np.where(np.logical_or(lr_y < 0.25, lr_y > 0.75))[0]\n",
    "idx_middle = np.where(np.logical_and(lr_y > 0.25, lr_y < 0.75))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(true_y[idx_end], lr_y[idx_end]), roc_auc_score(true_y[idx_end], lstm_y[idx_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(true_y[idx_middle], lr_y[idx_middle]), roc_auc_score(true_y[idx_middle], lstm_y[idx_middle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import generate_latex_tables\n",
    "keys_to_use = ['roc_auc', 'pr_auc']\n",
    "generate_latex_tables(data, keys_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'outputs/Readmission/Basic/'\n",
    "exps = os.listdir(dirname)\n",
    "for e in sorted(exps) :\n",
    "    if 'Structured' in e :\n",
    "        print(e)\n",
    "        print_results_from_model(get_latest_model(os.path.join(dirname, e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'outputs/Diagnosis/Basic/'\n",
    "exps = os.listdir(dirname)\n",
    "for e in sorted(exps) :\n",
    "    if 'Structured' in e :\n",
    "        print(e)\n",
    "        print_results_from_model(get_latest_model(os.path.join(dirname, e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
