{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import readmission_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readmission_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Experiments\n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Comparison\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_attention_config = experiments[3](data, structured=True)\n",
    "lstm_attn_eval = Evaluator(BasicCT, get_latest_model(os.path.join('outputs', lstm_attention_config['exp_config']['exp_name'])))\n",
    "dev_data = data.get_data('dev', structured=True)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)\n",
    "lstm_outputs = lstm_attn_eval.evaluate(dev_data, save_results=False)\n",
    "del lstm_attn_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_structured_attention_config = structured_experiments[1](data, structured=True, encodings=data.structured_columns)\n",
    "lstm_struct_attn_eval = Evaluator(BasicCT, \n",
    "                                  get_latest_model(os.path.join('outputs', lstm_structured_attention_config['exp_config']['exp_name'])))\n",
    "dev_data = data.get_data('dev', structured=True, encodings=data.structured_columns)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)\n",
    "lstm_struct_outputs = lstm_struct_attn_eval.evaluate(dev_data, save_results=False)\n",
    "del lstm_struct_attn_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_comparison import get_comparative_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = []\n",
    "for x, a1, a2 in zip(dev_data.X, lstm_outputs['attentions'], lstm_struct_outputs['attentions']) :\n",
    "    measures.append(get_comparative_measures(x, a1, a2))\n",
    "measures = pd.DataFrame(measures)\n",
    "y = np.array(dev_data.y)[:, 0]\n",
    "measures['y'] = y\n",
    "\n",
    "words_A = [[data.vocab.idx2word[x] for x in d] for d in list(measures['A'])]\n",
    "measures['A'] = words_A\n",
    "\n",
    "words_B = [[data.vocab.idx2word[x] for x in d] for d in list(measures['B'])]\n",
    "measures['B'] = words_B\n",
    "\n",
    "measures_0 = measures['y'] == 0\n",
    "measures_1 = measures['y'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_max_attentions = [max(x) for x in lstm_outputs['attentions']]\n",
    "lstm_struct_max_attentions = [max(x) for x in lstm_struct_outputs['attentions']]\n",
    "_ = plt.hist([lstm_max_attentions, lstm_struct_max_attentions], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(measures['len'], measures['haus'], s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(measures[measures_0]['emd'], bins=30)\n",
    "_ = plt.hist(measures[measures_1]['emd'], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_comparison import get_comparative_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [measures_0, measures_1] :\n",
    "    plt.hist([measures[c]['A&B'], measures[c]['A-B'], measures[c]['B-A']], label=['A&B', 'A-B', 'B-A'])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [measures_0, measures_1] :\n",
    "    plt.hist([measures[c]['jacc']], label=['jacc'], bins=30)\n",
    "    plt.show()\n",
    "    plt.scatter(measures[c]['jacc'], measures[c]['A|B'], s=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.get_data('train', structured=True)\n",
    "dev_data = data.get_data('dev', structured=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.models.baselines.LR import LR, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR({'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name})\n",
    "lr.train(train_data)\n",
    "lr.evaluate(dev_data, save_results=True)\n",
    "# lr.get_features(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA({'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name})\n",
    "lda.train(train_data)\n",
    "lda.evaluate(dev_data, save_results=True)\n",
    "print(lda.get_topics(n=10))\n",
    "topics = lda.get_topics(n=10)\n",
    "print([topics[i] for i in np.argsort(lda.lda_classifier.coef_[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[topics[i] for i in np.argsort(lda.lda_classifier.coef_[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = outputs['predictions'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import collapse_and_print_word_attn, print_sent_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "collapse_and_print_word_attn(data.vocab, dev_data.X[n], outputs['word_attentions'][n])\n",
    "print_sent_attn(data.vocab, dev_data.X[n], outputs['sentence_attentions'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['sentence_attentions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = [kendalltau(range(len(outputs['sentence_attentions'][i])), outputs['sentence_attentions'][i]) \n",
    "         for i in range(len(outputs['sentence_attentions']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho, leng = zip(*[(x[0], y) for x, y in zip(corrs, [len(z) for z in outputs['sentence_attentions']]) if x[0] == x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rho, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval, leng1 = zip(*[(x[1], y) for x, y in zip(corrs, [len(z) for z in outputs['sentence_attentions']]) if x[1] == x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import generate_latex_tables\n",
    "keys_to_use = ['1/precision', '1/recall', '1/f1-score', 'accuracy', 'roc_auc', 'pr_auc']\n",
    "generate_latex_tables(data, keys_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
