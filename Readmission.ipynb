{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Run Diagnosis experiments')\n",
    "parser.add_argument(\"--data_dir\", type=str, required=True)\n",
    "parser.add_argument('--display', dest='display', action='store_true')\n",
    "parser.add_argument(\"--output_dir\", type=str)\n",
    "parser.add_argument(\"--mock\", dest='mock', action='store_true')\n",
    "\n",
    "args = parser.parse_args(['--data_dir=.', '--output_dir=outputs/', '--display'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import readmission_dataset\n",
    "data = readmission_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.Experiments.training_exps import get_basic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data = get_basic_data(data, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.generate_bowder(train_data, stop_words=True, norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.X = data.get_vec_encoding(train_data, _type='tfidf')\n",
    "dev_data.X = data.get_vec_encoding(dev_data, _type='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.X = np.array(train_data.X.todense())\n",
    "dev_data.X = np.array(dev_data.X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.Experiments.configs import vector_experiment\n",
    "config = vector_experiment(data)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer, Evaluator\n",
    "from models.Baseline import ClassificationTrainer as BaseCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(BaseCT, config, _type=data.metrics_type, display_metrics=args.display)\n",
    "trainer.train(train_data, dev_data, save_on_metric=data.save_on_metric)\n",
    "\n",
    "evaluator = Evaluator(BaseCT, trainer.model.dirname, _type=data.metrics_type, display_metrics=args.display)\n",
    "_ = evaluator.evaluate(dev_data, save_results=True)\n",
    "print('='*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.models.baselines.LR import LR, LDA\n",
    "\n",
    "train_data, dev_data = get_basic_data(data, truncate=100)\n",
    "lr = LR({'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name, 'type' : 'classifier', 'norm' : 'l2'})\n",
    "lr.train(train_data)\n",
    "lr.evaluate(dev_data, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.print_all_features(n=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Comparison\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_attention_config = experiments[3](data, structured=True)\n",
    "lstm_attn_eval = Evaluator(BasicCT, get_latest_model(os.path.join('outputs', lstm_attention_config['exp_config']['exp_name'])))\n",
    "dev_data = data.get_data('dev', structured=True)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)\n",
    "lstm_outputs = lstm_attn_eval.evaluate(dev_data, save_results=False)\n",
    "del lstm_attn_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_structured_attention_config = structured_experiments[1](data, structured=True, encodings=data.structured_columns)\n",
    "lstm_struct_attn_eval = Evaluator(BasicCT, \n",
    "                                  get_latest_model(os.path.join('outputs', lstm_structured_attention_config['exp_config']['exp_name'])))\n",
    "dev_data = data.get_data('dev', structured=True, encodings=data.structured_columns)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)\n",
    "lstm_struct_outputs = lstm_struct_attn_eval.evaluate(dev_data, save_results=False)\n",
    "del lstm_struct_attn_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_comparison import get_comparative_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = []\n",
    "for x, a1, a2 in zip(dev_data.X, lstm_outputs['attentions'], lstm_struct_outputs['attentions']) :\n",
    "    measures.append(get_comparative_measures(x, a1, a2))\n",
    "measures = pd.DataFrame(measures)\n",
    "y = np.array(dev_data.y)[:, 0]\n",
    "measures['y'] = y\n",
    "\n",
    "words_A = [[data.vocab.idx2word[x] for x in d] for d in list(measures['A'])]\n",
    "measures['A'] = words_A\n",
    "\n",
    "words_B = [[data.vocab.idx2word[x] for x in d] for d in list(measures['B'])]\n",
    "measures['B'] = words_B\n",
    "\n",
    "measures_0 = measures['y'] == 0\n",
    "measures_1 = measures['y'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_max_attentions = [max(x) for x in lstm_outputs['attentions']]\n",
    "lstm_struct_max_attentions = [max(x) for x in lstm_struct_outputs['attentions']]\n",
    "_ = plt.hist([lstm_max_attentions, lstm_struct_max_attentions], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(measures['len'], measures['haus'], s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(measures[measures_0]['emd'], bins=30)\n",
    "_ = plt.hist(measures[measures_1]['emd'], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_comparison import get_comparative_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [measures_0, measures_1] :\n",
    "    plt.hist([measures[c]['A&B'], measures[c]['A-B'], measures[c]['B-A']], label=['A&B', 'A-B', 'B-A'])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [measures_0, measures_1] :\n",
    "    plt.hist([measures[c]['jacc']], label=['jacc'], bins=30)\n",
    "    plt.show()\n",
    "    plt.scatter(measures[c]['jacc'], measures[c]['A|B'], s=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA({'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name})\n",
    "lda.train(train_data)\n",
    "lda.evaluate(dev_data, save_results=True)\n",
    "print(lda.get_topics(n=10))\n",
    "topics = lda.get_topics(n=10)\n",
    "print([topics[i] for i in np.argsort(lda.lda_classifier.coef_[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[topics[i] for i in np.argsort(lda.lda_classifier.coef_[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = outputs['predictions'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import collapse_and_print_word_attn, print_sent_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "collapse_and_print_word_attn(data.vocab, dev_data.X[n], outputs['word_attentions'][n])\n",
    "print_sent_attn(data.vocab, dev_data.X[n], outputs['sentence_attentions'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['sentence_attentions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = [kendalltau(range(len(outputs['sentence_attentions'][i])), outputs['sentence_attentions'][i]) \n",
    "         for i in range(len(outputs['sentence_attentions']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho, leng = zip(*[(x[0], y) for x, y in zip(corrs, [len(z) for z in outputs['sentence_attentions']]) if x[0] == x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rho, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval, leng1 = zip(*[(x[1], y) for x, y in zip(corrs, [len(z) for z in outputs['sentence_attentions']]) if x[1] == x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import generate_latex_tables\n",
    "keys_to_use = ['1/precision', '1/recall', '1/f1-score', 'accuracy', 'roc_auc', 'pr_auc']\n",
    "generate_latex_tables(data, keys_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5, 10, 20)\n",
    "b = torch.randn(100, 1, 10)\n",
    "c = torch.randn(100, 50, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size 100 at dimension 0, but got size 5 for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e78719f7e731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size 100 at dimension 0, but got size 5 for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "torch.bmm(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
