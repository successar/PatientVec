{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(name='Readmission', dirname='preprocess/Readmission/')\n",
    "labellist = ['y']\n",
    "data.generate_labels(labellist, len(labellist), 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.generate_encoded_field('gender_y', 'onehot')\n",
    "data.generate_encoded_field('age_y', 'onehot')\n",
    "data.generate_encoded_field('ethnicity_y', 'onehot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in data.dataframe.columns if x.startswith('feature')]\n",
    "for f in features :\n",
    "    data.generate_encoded_field(f, 'trivial')\n",
    "    \n",
    "data.set_structured_params(regexs=[r'^feature', 'gender_y', 'age_y', 'ethnicity_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Vanilla import ClassificationTrainer as BasicCT\n",
    "from models.hierarchical import ClassificationTrainer as HierCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experiments.experiments import experiments, hierarchical_experiments, structured_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Experiments\n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.get_data('train', structured=True)\n",
    "dev_data = data.get_data('dev', structured=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.filter_data_length(train_data, truncate=90)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in experiments :\n",
    "    config = e(data, structured=False)\n",
    "    print(config)\n",
    "    trainer = Trainer(BasicCT, config)\n",
    "    trainer.train(train_data, dev_data)\n",
    "\n",
    "    evaluator = Evaluator(BasicCT, trainer.model.dirname)\n",
    "    _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "    print('='*30)\n",
    "    \n",
    "for e in hierarchical_experiments :\n",
    "    config = e(data, structured=False)\n",
    "    print(config)\n",
    "    trainer = Trainer(HierCT, config)\n",
    "    trainer.train(train_data, dev_data)\n",
    "\n",
    "    evaluator = Evaluator(HierCT, trainer.model.dirname)\n",
    "    _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in experiments :\n",
    "    config = e(data, structured=True)\n",
    "    print(config)\n",
    "    trainer = Trainer(BasicCT, config)\n",
    "    trainer.train(train_data, dev_data)\n",
    "\n",
    "    evaluator = Evaluator(BasicCT, trainer.model.dirname)\n",
    "    _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "    print('='*30)\n",
    "    \n",
    "for e in hierarchical_experiments :\n",
    "    config = e(data, structured=True)\n",
    "    print(config)\n",
    "    trainer = Trainer(HierCT, config)\n",
    "    trainer.train(train_data, dev_data)\n",
    "\n",
    "    evaluator = Evaluator(HierCT, trainer.model.dirname)\n",
    "    _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.get_data('train', structured=True, encodings=['gender_y', 'ethnicity_y', 'age_y'])\n",
    "dev_data = data.get_data('dev', structured=True, encodings=['gender_y', 'ethnicity_y', 'age_y'])\n",
    "\n",
    "train_data = data.filter_data_length(train_data, truncate=90)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in structured_experiments :\n",
    "    config = e(data, structured=True, encodings=['gender_y', 'ethnicity_y', 'age_y'])\n",
    "    print(config)\n",
    "    trainer = Trainer(BasicCT, config)\n",
    "    trainer.train(train_data, dev_data)\n",
    "\n",
    "    evaluator = Evaluator(BasicCT, trainer.model.dirname)\n",
    "    _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.get_data('train', structured=True, encodings=['gender_y', 'ethnicity_y', 'age_y'])\n",
    "dev_data = data.get_data('dev', structured=True, encodings=['gender_y', 'ethnicity_y', 'age_y'])\n",
    "\n",
    "train_data = data.filter_data_length(train_data, truncate=90)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)\n",
    "\n",
    "for e in structured_experiments :\n",
    "    config = e(data, structured=False, encodings=['gender_y', 'ethnicity_y', 'age_y'])\n",
    "    print(config)\n",
    "    trainer = Trainer(BasicCT, config)\n",
    "    trainer.train(train_data, dev_data)\n",
    "\n",
    "    evaluator = Evaluator(BasicCT, trainer.model.dirname)\n",
    "    _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.get_data('train', structured=True, encodings=data.structured_columns)\n",
    "dev_data = data.get_data('dev', structured=True, encodings=data.structured_columns)\n",
    "train_data = data.filter_data_length(train_data, truncate=90)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)\n",
    "\n",
    "for e in structured_experiments :\n",
    "    config = e(data, structured=True, encodings=data.structured_columns)\n",
    "    print(config)\n",
    "    \n",
    "    trainer = Trainer(BasicCT, config)\n",
    "    trainer.train(train_data, dev_data)\n",
    "\n",
    "    evaluator = Evaluator(BasicCT, trainer.model.dirname)\n",
    "    _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.get_data('train', structured=True, encodings=data.structured_columns)\n",
    "dev_data = data.get_data('dev', structured=True, encodings=data.structured_columns)\n",
    "train_data = data.filter_data_length(train_data, truncate=90)\n",
    "dev_data = data.filter_data_length(dev_data, truncate=90)\n",
    "\n",
    "for e in structured_experiments :\n",
    "    config = e(data, structured=False, encodings=data.structured_columns)\n",
    "    print(config)\n",
    "    \n",
    "    trainer = Trainer(BasicCT, config)\n",
    "    trainer.train(train_data, dev_data)\n",
    "\n",
    "    evaluator = Evaluator(BasicCT, trainer.model.dirname)\n",
    "    _ = evaluator.evaluate(dev_data, save_results=True)\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.get_data('train', structured=True)\n",
    "dev_data = data.get_data('dev', structured=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.models.baselines.LR import LR, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR({'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name})\n",
    "lr.train(train_data)\n",
    "lr.evaluate(dev_data, save_results=True)\n",
    "# lr.get_features(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA({'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name})\n",
    "lda.train(train_data)\n",
    "lda.evaluate(dev_data, save_results=True)\n",
    "print(lda.get_topics(n=10))\n",
    "topics = lda.get_topics(n=10)\n",
    "print([topics[i] for i in np.argsort(lda.lda_classifier.coef_[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[topics[i] for i in np.argsort(lda.lda_classifier.coef_[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = outputs['predictions'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import collapse_and_print_word_attn, print_sent_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "collapse_and_print_word_attn(data.vocab, dev_data.X[n], outputs['word_attentions'][n])\n",
    "print_sent_attn(data.vocab, dev_data.X[n], outputs['sentence_attentions'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['sentence_attentions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = [kendalltau(range(len(outputs['sentence_attentions'][i])), outputs['sentence_attentions'][i]) \n",
    "         for i in range(len(outputs['sentence_attentions']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho, leng = zip(*[(x[0], y) for x, y in zip(corrs, [len(z) for z in outputs['sentence_attentions']]) if x[0] == x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rho, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval, leng1 = zip(*[(x[1], y) for x, y in zip(corrs, [len(z) for z in outputs['sentence_attentions']]) if x[1] == x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for e in experiments :\n",
    "    config = e(data, structured=False)\n",
    "    filename = config['exp_config']['exp_name']\n",
    "    filename = os.path.join('outputs/classification/', filename)\n",
    "    push_latest_model(filename, config['exp_config']['exp_name'])\n",
    "    \n",
    "    config = e(data, structured=True)\n",
    "    filename = config['exp_config']['exp_name']\n",
    "    filename = os.path.join('outputs/classification/', filename)\n",
    "    push_latest_model(filename, config['exp_config']['exp_name'])\n",
    "    \n",
    "for e in hierarchical_experiments :\n",
    "    config = e(data, structured=False)\n",
    "    filename = config['exp_config']['exp_name']\n",
    "    filename = os.path.join('outputs/hierarchical_classification/', filename)\n",
    "    push_latest_model(filename, config['exp_config']['exp_name'])\n",
    "    \n",
    "    config = e(data, structured=True)\n",
    "    filename = config['exp_config']['exp_name']\n",
    "    filename = os.path.join('outputs/hierarchical_classification/', filename)\n",
    "    push_latest_model(filename, config['exp_config']['exp_name'])\n",
    "    \n",
    "for e in structured_experiments :\n",
    "    config = e(data, structured=True, encodings=['gender_y', 'ethnicity_y', 'age_y'])\n",
    "    filename = config['exp_config']['exp_name']\n",
    "    filename = os.path.join('outputs/classification/', filename)\n",
    "    push_latest_model(filename, config['exp_config']['exp_name'])\n",
    "    \n",
    "    config = e(data, structured=False, encodings=['gender_y', 'ethnicity_y', 'age_y'])\n",
    "    filename = config['exp_config']['exp_name']\n",
    "    filename = os.path.join('outputs/classification/', filename)\n",
    "    push_latest_model(filename, config['exp_config']['exp_name'])\n",
    "    \n",
    "    config = e(data, structured=True, encodings=data.structured_columns)\n",
    "    filename = config['exp_config']['exp_name']\n",
    "    filename = os.path.join('outputs/classification/', filename)\n",
    "    push_latest_model(filename, config['exp_config']['exp_name'])\n",
    "    \n",
    "    config = e(data, structured=False, encodings=data.structured_columns)\n",
    "    filename = config['exp_config']['exp_name']\n",
    "    filename = os.path.join('outputs/classification/', filename)\n",
    "    push_latest_model(filename, config['exp_config']['exp_name'])\n",
    "    \n",
    "    \n",
    "for e in os.listdir('outputs/baselines/Readmission_y/baselines/') :\n",
    "    filename = os.path.join('outputs/baselines/Readmission_y/baselines/', e)\n",
    "    push_latest_model(filename, os.path.join('Readmission_y/baselines/', e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Readmission_y'\n",
    "dataset_path = os.path.join('latex_evals', dataset)\n",
    "output_path = os.path.join('Text-encoding-EHR/results/', dataset)\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir(dataset_path)\n",
    "keys_to_use = ['1/precision', '1/recall', '1/f1-score', 'accuracy', 'roc_auc', 'pr_auc']\n",
    "for d in dirs :\n",
    "    subpath = os.path.join(dataset_path, d)\n",
    "    output_file = os.path.join(output_path, d + '.csv')\n",
    "    dfs = []\n",
    "    for f in sorted(os.listdir(subpath)) :\n",
    "        if os.path.isfile(os.path.join(subpath, f)) :\n",
    "            d = json.load(open(os.path.join(subpath, f)))\n",
    "            results = {k:d['results'][k] for k in keys_to_use}\n",
    "            results['Method'] = f[:-14].replace('+', ' +').replace('_', ':')\n",
    "            dfs.append(pd.DataFrame([results]))\n",
    "        else :\n",
    "            logging.error(\"%s not a file\", f)\n",
    "    \n",
    "    dfs = pd.concat(dfs)\n",
    "    dfs.to_csv(output_file, columns=['Method'] + keys_to_use, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
