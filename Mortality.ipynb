{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import *\n",
    "from PatientVec.Experiments.hyperparam_exps import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Run Diagnosis experiments')\n",
    "parser.add_argument(\"--data_dir\", type=str, required=True)\n",
    "parser.add_argument('--display', dest='display', action='store_true')\n",
    "parser.add_argument(\"--output_dir\", type=str)\n",
    "parser.add_argument(\"--mock\", dest='mock', action='store_true')\n",
    "\n",
    "args = parser.parse_args(['--data_dir=.', '--output_dir=outputs/', '--display'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import mortality_dataset\n",
    "data = mortality_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data = get_basic_data(data, structured=True, truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auc = {}\n",
    "results_pr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auc_l1 = {}\n",
    "results_pr_l1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [1, 2, 3] :\n",
    "    config = {'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name, \n",
    "              'type' : data.metrics_type, 'norm' : 'l'+str(l), 'constant_mul' : 1.0}\n",
    "    lr = LR(config)\n",
    "    lr.train(train_data)\n",
    "    metrics = lr.evaluate(dev_data, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = attention_configs[1]\n",
    "structured = True\n",
    "config = e(data, structured=structured, args=args)\n",
    "if args.output_dir is not None :\n",
    "    config['exp_config']['basepath'] = args.output_dir\n",
    "if hasattr(args, 'modify_config') :\n",
    "    config = args.modify_config(config)\n",
    "print(config)\n",
    "\n",
    "trainer = Trainer(BasicCT, config, _type=data.metrics_type, display_metrics=args.display)\n",
    "trainer.train(train_data, dev_data, save_on_metric=data.save_on_metric)\n",
    "\n",
    "evaluator = Evaluator(BasicCT, trainer.model.dirname, _type=data.metrics_type, display_metrics=args.display)\n",
    "_ = evaluator.evaluate(dev_data, save_results=True)\n",
    "print('='*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_models = {}\n",
    "for c in [1.0, 2.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0, 10000, 100000] :\n",
    "    config = {'vocab' : data.vocab, 'stop_words' : True, 'exp_name' : data.name, \n",
    "              'type' : data.metrics_type, 'norm' : 'l1', 'constant_mul' : c}\n",
    "    lr = LR(config)\n",
    "    lr.train(train_data)\n",
    "    metrics = lr.evaluate(dev_data, save_results=True)\n",
    "    results_auc_l1[c] = metrics['roc_auc']\n",
    "    results_pr_l1[c] = metrics['pr_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logx = list(results_auc.keys())\n",
    "plt.plot(logx, list(results_auc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pr_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_feat = lr_models[1].bow_classifier.estimators_[0].coef_[0]\n",
    "l2_feat = lr_models[2].bow_classifier.estimators_[0].coef_[0]\n",
    "l3_feat = lr_models[3].bow_classifier.estimators_[0].coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(sorted(l1_feat))\n",
    "sns.kdeplot(sorted(l2_feat))\n",
    "sns.kdeplot(sorted(l3_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = [0] * data.vocab.vocab_size\n",
    "for i, v in data.vocab.idx2word.items() :\n",
    "    vc[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.Experiments.modifiable_config_exp import attention_configs\n",
    "avg_attn_config = attention_configs[0](data, structured=True, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_attn_config['model']['embedder']['type'] = 'elmo_embedder'\n",
    "del avg_attn_config['model']['embedder']['embedding_file']\n",
    "avg_attn_config['model']['embedder']['elmo_options'] = {\n",
    "    'options_file' : '../../elmo_2x4096_512_2048cnn_2xhighway_options.json',\n",
    "    'weight_file' : '../../elmo_2x4096_512_2048cnn_2xhighway_weights_PubMed_only.hdf5',\n",
    "    'vocab_to_cache' : vc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_attn_config['training_config']['common']['bsize'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.models.Vanilla import ClassificationTrainer as BasicCT\n",
    "from PatientVec.trainer import Trainer, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging.config\n",
    "logging.config.dictConfig({\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = data.filter_data_length(data.get_data('dev', structured=True), 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.filter_data_length(data.get_data('train', structured=True), 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(BasicCT, avg_attn_config, _type=data.metrics_type, display_metrics=args.display)\n",
    "trainer.train(train_data, dev_data, save_on_metric=data.save_on_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PatientVec.Experiments.evaluate import get_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = get_evaluator(data, 'Attention/Average(hs=256)+Attention(additive)(hs=128)+Structured/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = evaluator.evaluate(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_attentions = [max(x) for x in output['attentions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(max_attentions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import generate_latex_tables\n",
    "keys_to_use = ['roc_auc', 'pr_auc']\n",
    "generate_latex_tables(data, keys_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'outputs/Readmission/Basic/'\n",
    "exps = os.listdir(dirname)\n",
    "for e in sorted(exps) :\n",
    "    if 'Structured' in e :\n",
    "        print(e)\n",
    "        print_results_from_model(get_latest_model(os.path.join(dirname, e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'outputs/Diagnosis/Basic/'\n",
    "exps = os.listdir(dirname)\n",
    "for e in sorted(exps) :\n",
    "    if 'Structured' in e :\n",
    "        print(e)\n",
    "        print_results_from_model(get_latest_model(os.path.join(dirname, e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
